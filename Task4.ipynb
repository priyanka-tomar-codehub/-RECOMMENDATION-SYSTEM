import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import svds
from sklearn.metrics import mean_squared_error, mean_absolute_error
from math import sqrt
import matplotlib.pyplot as plt
import time

# -----------------------------
# User-editable paths & params
# -----------------------------
rating_csv_path = r"C:/Users/Priyanka/Downloads/archive (4)/rating.csv"
movie_csv_path = r"C:/Users/Priyanka/Downloads/archive (4)/movie.csv"

k = 10               # latent factors (reduce for speed/memory)
test_frac = 0.01     # fraction of ratings to use as test set for evaluation
top_n = 10           # number of recommendations to return
random_state = 42

# -----------------------------
# 1. Load datasets
# -----------------------------
print("Loading datasets...")
ratings = pd.read_csv(rating_csv_path)
movies = pd.read_csv(movie_csv_path)
print(f"Ratings shape: {ratings.shape}")
print(f"Movies shape: {movies.shape}")

# Basic validation
assert {'userId', 'movieId', 'rating'}.issubset(ratings.columns), "ratings CSV must contain userId, movieId, rating"
assert 'title' in movies.columns and 'movieId' in movies.columns, "movies CSV must contain movieId and title"

# -----------------------------
# 2. Re-map IDs to 0..N-1 (dense contiguous indices)
# -----------------------------
print("Re-mapping IDs to contiguous indices...")
unique_user_ids = ratings['userId'].unique()
unique_movie_ids = ratings['movieId'].unique()
user_mapping = {uid: idx for idx, uid in enumerate(unique_user_ids)}
movie_mapping = {mid: idx for idx, mid in enumerate(unique_movie_ids)}
inv_movie_mapping = {v: k for k, v in movie_mapping.items()}  # movie index -> original movieId

ratings['user_index'] = ratings['userId'].map(user_mapping)
ratings['movie_index'] = ratings['movieId'].map(movie_mapping)

num_users = len(user_mapping)
num_items = len(movie_mapping)
print(f"num_users={num_users}, num_items={num_items}")

# -----------------------------
# 3. Build sparse user-item matrix
# -----------------------------
print("Building sparse user-item matrix...")
R_sparse = csr_matrix((ratings['rating'], (ratings['user_index'], ratings['movie_index'])),
                      shape=(num_users, num_items))
print("Sparse matrix built.")

# -----------------------------
# 4. Demean (subtract user mean) on sparse structure
# -----------------------------
print("Computing user means and demeaning sparse matrix...")
user_ratings_sum = R_sparse.sum(axis=1).A1                  # sum per user
user_counts = (R_sparse != 0).sum(axis=1).A1                 # non-zero counts per user
user_ratings_mean = user_ratings_sum / np.maximum(user_counts, 1)

# Create a demeaned sparse matrix (only non-zero entries changed)
R_demeaned = R_sparse.copy().astype(float)
for i in range(R_demeaned.shape[0]):
    start, end = R_demeaned.indptr[i], R_demeaned.indptr[i+1]
    if end > start:
        R_demeaned.data[start:end] -= user_ratings_mean[i]

print("Done demeaning.")

# -----------------------------
# 5. Compute sparse SVD (svds)
# -----------------------------
print(f"Running sparse SVD with k={k} (this may take a little while)...")
start_time = time.time()
U, sigma_vals, Vt = svds(R_demeaned, k=k)
end_time = time.time()
print(f"SVD done in {end_time - start_time:.1f} sec. shapes: U={U.shape}, sigma={sigma_vals.shape}, Vt={Vt.shape}")

# Sort sigma in descending order (svds returns ascending)
idx_desc = np.argsort(sigma_vals)[::-1]
sigma_vals = sigma_vals[idx_desc]
U = U[:, idx_desc]
Vt = Vt[idx_desc, :]

sigma = np.diag(sigma_vals)
U_sigma = np.dot(U, sigma)   # user-factor matrix multiplied by sigma (size: num_users x k)

# -----------------------------
# 6. Prediction helpers (memory efficient)
# -----------------------------

def predict_single(user_idx, movie_idx):
    """Predict a single user-movie rating without building full dense matrix."""
    return float(np.dot(U_sigma[user_idx, :], Vt[:, movie_idx]) + user_ratings_mean[user_idx])

def predict_user_scores_vectorized(user_idx):
    """Return predicted scores for all movies for a given user (vectorized)."""
    return np.dot(U_sigma[user_idx, :], Vt) + user_ratings_mean[user_idx]

# -----------------------------
# 7. Evaluation (RMSE, MAE) on small sample
# -----------------------------
print("Evaluating on a random test sample...")
test = ratings.sample(frac=test_frac, random_state=random_state)
true_ratings = test['rating'].values
preds = []
for _, row in test.iterrows():
    preds.append(predict_single(int(row['user_index']), int(row['movie_index'])))

rmse = sqrt(mean_squared_error(true_ratings, preds))
mae = mean_absolute_error(true_ratings, preds)
print(f"Evaluation on {len(test)} samples -> RMSE: {rmse:.4f}, MAE: {mae:.4f}")

# -----------------------------
# 8. Fast Top-N recommendation (vectorized + argpartition)
# -----------------------------
print("Preparing fast top-N recommendation function...")
# Precompute a mapping from user -> set of rated movie indices for quick exclusion
user_rated_movies = ratings.groupby('user_index')['movie_index'].apply(set).to_dict()


def recommend_movies(user_id, n=10):
    """Return top-n recommendations (movieId, title, score) for original userId."""
    if user_id not in user_mapping:
        return []
    user_idx = user_mapping[user_id]

    # predicted scores vector for all movies
    scores = predict_user_scores_vectorized(user_idx)

    # candidate indices (all movie indices except those already rated)
    rated = user_rated_movies.get(user_idx, set())
    # If user has rated almost all movies, fallback: include all then filter
    candidate_mask = np.ones(num_items, dtype=bool)
    if len(rated) > 0:
        candidate_mask[list(rated)] = False

    # Efficient top-n selection among candidates
    candidate_indices = np.nonzero(candidate_mask)[0]
    if len(candidate_indices) == 0:
        return []

    if len(candidate_indices) <= n:
        top_candidate_idxs = candidate_indices
    else:
        part = np.argpartition(scores[candidate_indices], -n)[-n:]
        top_candidate_idxs = candidate_indices[part]
        # sort them
        top_candidate_idxs = top_candidate_idxs[np.argsort(scores[top_candidate_idxs])[::-1]]

    recs = []
    for midx in top_candidate_idxs:
        orig_mid = inv_movie_mapping[midx]
        title_arr = movies[movies['movieId'] == orig_mid]['title'].values
        title = title_arr[0] if len(title_arr) > 0 else str(orig_mid)
        recs.append((orig_mid, title, float(scores[midx])))

    return recs

# -----------------------------
# 9. Demo: Show recommendations for some example users and plot scores
# -----------------------------
example_users = list(ratings['userId'].drop_duplicates().sample(3, random_state=1))
print(f"Showing top-{top_n} recommendations for example users: {example_users}")
for uid in example_users:
    recs = recommend_movies(uid, n=top_n)
    print(f"\nUser {uid} -> Top {top_n} recommendations:")
    for i, (mid, title, score) in enumerate(recs, 1):
        print(f" {i}. {title} (movieId={mid}) â€” predicted: {score:.2f}")

# Plot predicted score distribution for first example user (top 50)
if len(example_users) > 0:
    u0 = example_users[0]
    scores = predict_user_scores_vectorized(user_mapping[u0])
    # exclude rated
    rated = user_rated_movies.get(user_mapping[u0], set())
    candidate_mask = np.ones(num_items, dtype=bool)
    if len(rated) > 0:
        candidate_mask[list(rated)] = False
    candidate_indices = np.nonzero(candidate_mask)[0]
    top_k = 50
    if len(candidate_indices) > 0:
        part = np.argpartition(scores[candidate_indices], -top_k)[-top_k:]
        top_candidate_idxs = candidate_indices[part]
        top_candidate_idxs = top_candidate_idxs[np.argsort(scores[top_candidate_idxs])[::-1]]
        titles = [movies[movies['movieId'] == inv_movie_mapping[idx]]['title'].values[0] if len(movies[movies['movieId'] == inv_movie_mapping[idx]]['title'].values) > 0 else str(inv_movie_mapping[idx]) for idx in top_candidate_idxs]
        top_scores = scores[top_candidate_idxs]

        plt.figure(figsize=(10,6))
        plt.barh(range(len(top_scores))[::-1], top_scores)
        plt.yticks(range(len(top_scores))[::-1], [t[:50] for t in titles])
        plt.xlabel('Predicted rating')
        plt.title(f'Top {len(top_scores)} predicted scores for user {u0}')
        plt.tight_layout()
        plt.show()

# -----------------------------
# 10. Save a small CSV of sample recommendations for submission
# -----------------------------
print("Saving sample recommendations to 'sample_recommendations.csv'...")
rows = []
for uid in example_users:
    for mid, title, score in recommend_movies(uid, n=top_n):
        rows.append({'userId': uid, 'movieId': mid, 'title': title, 'predicted_rating': score})
pd.DataFrame(rows).to_csv('sample_recommendations.csv', index=False)
print("Done. CSV saved.")

# -----------------------------
# End of script
# -----------------------------
print("All finished. You can include this script (or convert to a Jupyter notebook) as your deliverable.")
